\section{Future Work} 
\label{sec:future_work}

\tinyparagraph{Sorted and distributed key-value stores} Our fragmentation strategy is based on unsorted key-value store accesses with $\mathcal{O}(log)$ complexity. Neither our analysis, nor out  implementation EMFFrag, or our evaluation consider sorted key-value stores that allow to access sequential keys with constant time (scans). Neither did we consider distributed key-value stores which would allow parallel access. Key-value stores are easily distributed in peer-to-peer networks. This is done for two scaleability reasons: replication (allows more users to access the same data in less time) and sharding (distributes data to allow faster and larger storage). Fragmentation can have an influence on both.

\tinyparagraph{Transactions} If multiple user access/modify a model transactions become a necessity. Transaction can either be provided by the underlying data store (e.g. with Scalaris~\cite{ScalarisTransactions2008}) or can be implemented into EMFFrag. On non-distributed data stores, the usual transaction mechanisms can be implemented. More interesting is to explore the influence of fragmentation on transactions (and versioning), because fragmentation granularity also determines the maximum transaction granularity.

\tinyparagraph{Large value sets} In large models, single objects can become very large themselves if they hold large sets of attribute values and references. CDO maps an object's feature values to individual entries in a database table and can manage such objects, but does this slowly. EMFFrag (and Morsa), on the other hand, consider objects as atomic entities and large object can become a performance burden. We need to extend the fragmentation idea to large value sets. Similar to all consideration in this paper, strategies for large value sets have to be optimized and evaluated for the abstract tasks manipulation, iteration (traverse), indexed access (query), and range queries (partial load). 

\section{Conclusions}\label{sec:conclusions}

Large software models consist of up to $10^9$ objects. Models from other application can have a size of up to $10^{12}$ objects. Traversing models and loading larger aggregates of objects are common tasks (section~\ref{sec:applications}). Depending on fragment size, partially loading models can be done faster than loading whole models or loading models object by object. There is no optimal fragment size, but intermediate fragment sizes provide a good approximation  (sections~\ref{sec:gains} and~\ref{sec:evaluation}). We provide a persistence framework that allows automatic and transparent fragmentation, if appropriate containment features are marked as fragmentation points in the meta-model (sections~\ref{sec:fragmentation} and~\ref{sec:implemention}). We compared our framework to existing frameworks (EMF's XMI implementation, CDO and Morsa) and our framework performs significantly better for the tasks create/manipulate, traverse, and partial loads. Execution times are 5 to 10 times smaller. Model queries (that favor object-by-object based model persistence with indexes, such as in CDO and Morsa) can be executed with comparable execution times (section~\ref{sec:evaluation}). All together, fragmentation combines the advantages of both worlds, low memory usage and fast queries like with CDO or Morsa, and traverse and partial load execution times similar to those of XMI.   

Model fragmentation also determines the granularity of transactions, which can be a disadvantage. Further problems are single objects with features that can hold large value sets; the fragmentation approach has to be extended for fragmentation of such value sets (section~\ref{sec:future_work}). Our framework stores fragments in key-value stores. Those scale easily (both replication and sharding is supported) and integrate well with peer-to-peer computation schemes (e.g. map-reduce). Fragmentation is therefore a good preparation for modeling in the cloud applications (section~\ref{sec:related_work}). 